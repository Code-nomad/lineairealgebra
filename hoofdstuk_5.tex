\documentclass[lineaire_algebra_oplossingen.tex]{subfiles}
\begin{document}

\newpage
\part{Hoofdstuk 5}
\section{Bewijzen uit het boek}

\subsection{Stelling 5.2 p 177}
Zij $A\in \mathbb{R}^{n\times n}$ een vierkante matrix.
\subsubsection*{Te Bewijzen}
\begin{center}
Een getal $\lambda\in\mathbb{R}$ is een eigenwaarde van $A$.
\end{center}
\[\Leftrightarrow\]
\begin{center}
$\lambda$ is een nulpunt van de karakteristieke veelterm $det(X\mathbb{I}_n - A)$ van $A$
\end{center}
\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.
\begin{itemize}
\item $\Rightarrow$\\
Omdat $\lambda$ een eigenwaarde is van $A$ bestaat er een (eigen)vector $v$ zodat de volgende bewering geldt\footnote{Zie Definitie 5.1 p 177}.
\[
A\cdot v = \lambda v
\]
We weten dat $\lambda v =  \lambda \mathbb{I}_n \cdot v$ en dat de matrixvermenigvuldiging distributief is als ze bepaald is\footnote{Zie Eigenschappen 1.22 b}.
\[
A\cdot v - \lambda \mathbb{I}_n \cdot v = \vec{0} = (A-\lambda\mathbb{I}_n)\cdot v = \vec{0}
\]
Omdat $v$ per definitie geen nulvector is moet de determinant van $(A-\lambda\mathbb{I}_n)$ nul zijn opdat opdat $(A-\lambda\mathbb{I}_n)\cdot v = \vec{0}$ geldt.

\item $\Leftarrow$\\
Als $det(A-\lambda\mathbb{I}_n) = 0$ geldt voor $\lambda$ met $v$ als eigenvector, dan geldt zeker het volgende.
\[
(A-\lambda\mathbb{I}_n)\cdot v = \vec{0}
\]
\end{itemize}
\end{proof}

\subsection{Voorbeeld 5.4 p 178}
\subsubsection*{1)}
We zoeken nog een oplossing van de volgende vergelijking.
\[
\begin{pmatrix}
-1 & -1\\
-1 & -1\\
\end{pmatrix}
\cdot
\begin{pmatrix}
x\\y
\end{pmatrix}
=
\vec{0}
\]
De oplossingsverzameling hiervan is de volgende.
\[
V = \{-\lambda,\lambda|\lambda\in\mathbb{R}\}
\]
Als we nu $\lambda = 1$ kiezen krijgen we als eigenvector bij voorbeeld $(-1,1)$.

\subsubsection*{5)}
Voor elke eigenvector $v$ van $D$ geldt dat de afgeleide van $v$ een veelvoud is van $v$. Elke constante functie is een eigenvector en $0$ is de eigenwaarde voor die functies.

\subsection{Over Definitie 5.6 p 181}
Dit houdt in dat er voor $A$ een inverteerbare matrix $P$ bestaat zodat. $A = P^{-1}\cdot B\cdot P$ waarbij $B$ een diagonaalmatrix is.

\subsection{Stelling 5.7 p 181}
Zij $A \in \mathbb{R}^{n\times n}$ een vierkante matrix.
\subsubsection*{Te Bewijzen}
\begin{center}
$A$ is diagonaliseerbaar.
\end{center}
\[\Leftrightarrow\]
\begin{center}
$A$ heeft een basis die volledig bestaat uit eigenvectoren van $A$.
\end{center}
\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.
\begin{itemize}
\item $\Rightarrow$
Omdat $A$ diagonaliseerbaar is bestaat er een inverteerbare matrix $P$ zodat $A = P \cdot \Lambda\cdot P^{-1}$ geldt met $B$ een diagonaalmatrix. $\Lambda$ ziet er dus als volgt uit.
\[
\Lambda =
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0\\
0 & \lambda_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \lambda_n
\end{pmatrix}
\]
Nu geldt $A \cdot P = P \cdot \Lambda$ omwille van de definitie van inverteerbaarheid\footnote{Zie Definitie 1.27 p 34}.
We beschouwen $P$ nu als een rij van kolommen $P=(p_1 p_2 \cdots p_n)$
$A\cdot P$ ziet er dus als volgt uit.
\[
A\cdot P = A(p_1 p_2 \cdots p_n) = (Ap_1 Ap_2 \cdots Ap_n)
\]
Bovendien geldt de volgende bewering omdat $A \cdot P = P \cdot \Lambda$ geldt.
\[
A(p_1 p_2 \cdots p_n) = (Ap_1 Ap_2 \cdots Ap_n) = (p_1 p_2 \cdots p_n)\Lambda = (\lambda_1p_1 \lambda_2p_2 \cdots \lambda_np_n)
\]
Elk van de kolommen in $(\lambda_1p_1 \lambda_2p_2 \cdots \lambda_np_n)$ voldoet dus aan de volgende vergelijking omdat $\Lambda$ een diagonaalmatrix is.
\[
Ap_i = \lambda_ip_i
\]
Elke kolom van $P$ is bijgevolg een eigenvector van $A$ de eigenwaarden die horen bij deze eigenvectoren staan dan in de overeenkomstige kolommen op de diagonaal van $\Lambda$.
Nu moeten we nog bewijzen dat de kolommen van $P$ een basis vormen voor $\mathbb{R}^n$
Omdat $P$ precies $n$ kolommen bevat moeten we enkel bewijzen dat de kolommen van $P$ lineair onafhankelijk of dat ze voortbrengend zijn\footnote{Zie Stelling 3.41 p 109}. Omdat $P$ inverteerbaar is geldt dat de determinant van $P$ niet nul is\footnote{Zie Stelling 2.4 p 59}. Bijgevolg zijn de kolommen van $P$ lineair onafhankelijk\footnote{Stelling 2.2 p 57 en Zie Stelling 2.3 p 58} omdat $P$ rijreduceerbaar is tot een matrix zonder nulrijen.

\item $\Leftarrow$
$\mathbb{R}^n$ heeft een basis volledig bestaand uit eigenvectoren van $A$. Noem deze basis $\beta = \{\beta_1,\beta_2,...,\beta_n\}$. De vectoren uit deze basis voldoen dus aan de volgende bewering.
\[
A\cdot \beta_i = \lambda_i\beta_i
\]
$A$ beschouwen we als de matrix van een lineaire afbeelding ten opzichte van de standaard basis.
De matrix van basisverandering van de basis van de eigenvectoren naar de standaardbasis noemen we $P$. Dit is precies de matrix waarin de eigenvectoren van $A$ in kolommen staan.
$P^{-1}$ is nu de matrix van basisverandering van de standaardmatrix naar de basis van eigenvectoren. $P$ is zeker inverteerbaar omdat de kolommen lineair onafhankelijk zijn. We weten nu dat $A = PBP^{-1}$ geldt\footnote{Zie pagina 150 voor meer uitleg.}. Vermenigvuldig nu $P$ rechts aan beide kanten.
\[
AP = PB
\]
Beschouw $P = (\beta_1 \beta_2 \cdots \beta _n)$ als een rij van kolommen $\beta_i$. Beschouw $B = (b_1 b_2 \cdots b_n)$ als een kolom van rijen $b_i$.
\[
AP = (A\beta_1 A\beta_2 \cdots A\beta_n) = (\lambda_1\beta_1 \lambda_2\beta_2 \cdots \lambda_n\beta_n) = PB
\]
De tweede gelijkheid geldt omdat $\beta_i$ eigenvectoren zijn.
De derde gelijkheid kan enkel gelden als $B$ een diagonaalmatrix is. Sterker nog, $\lambda_i$, de eigenwaarden van $A$ staan precies op de diagonaal van $P$.

\end{itemize}
\end{proof}

\subsection{Stelling 5.8 p 182}
Zij $A,B \in \mathbb{R}^{n\times n}$ vierkante matrices met $B = P^{-1}AP$ zodat $A$ en $B$ gelijkvormig zijn.
\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item $A$ en $B$ hebben dezelfde karakteristieke veelterm.
\item $A$ en $B$ hebben dezelfde determinant.
\item $A$ en $B$ hebben hetzelfde spoor.
\end{enumerate}
\subsubsection*{Bewijs}
\begin{proof}
Direct bewijs.
\begin{enumerate}
\item 
\[
det(X\mathbb{I}_n - B) = det(X\mathbb{I}_n - P^{-1}AP) = det(XP^{-1}P - P^{-1}AP)
\]
Merk op dat $X$ een scalar is, geen vector.
We weten dat $XP^{-1}P = XP^{-1}IP P^{-1}XIP$\footnote{Zie Eigenschap 1.22 p 32 d)}. We kunnen dus verder gaan via gelijkheden.
\[
= det(P^{-1} XI P - P^{-1}AP) = det(P^{-1}\cdot ( XI P - AP)) = det(P^{-1}\cdot ( XI- A) \cdot P))
\]
De bovenstaande twee vergelijkingen gelden omwille van de distributiviteit van de matrixvermenigvuldiging ten opzichte van de matrix optelling\footnote{Zie Eigenschap 1.22 p 32 a) en b)}. De volgende gelijkheden gelden omwille van een eigenschap dan de determinantafbeelding\footnote{Zie Stelling 2.4 p 59 3} en de commutativiteit van de vermenigvuldiging in $\mathbb{R}$.
\[
=det(P^{-1})\cdot det( XI- A)\cdot det(P) =det( XI- A)\cdot det(P^{-1}) \cdot det(P)
\]
We weten dat $det( P^{-1}) = \frac{1}{det(P)}$\footnote{Zie Gevolg 2.5 p 60} dus de volgende gelijkheid geldt ook.
\[
=det( XI- A)\cdot \frac{1}{det(P)}\cdot det(P) = det( XI- A)
\]
Dat laatste rechterlid is precies de karakteristieke vergelijking van $A$.

\item
\[
B = P^{-1}AP
\]
\[
det(B) = det(P^{-1}AP) = det(P^{-1})\cdot det(A)\cdot det(P)
\]
\[
= det(A)\cdot det(P^{-1})\cdot det(P) = det(A)\cdot \frac{1}{det(P)}\cdot det(P) = det(A) \cdot 1 = det(A)
\]

\item
\[
B = P^{-1}AP 
\]
\[
Tr(B) = Tr(P^{-1}A P) = Tr(P^{-1}P A) = Tr(IA) = Tr(A)
\]
Bovenstaande gelijkheden gelden omwille van een eigenschap van het spoor van een product. Namelijk dat $Tr(AB) = Tr(BA)$ geldt\footnote{Zie opdracht 1.25 p 33}.
\end{enumerate}
\end{proof}

\subsection{Stelling 5.16 p 189}
Zij $L:V\rightarrow V$ een lineaire transformatie van de eindig dimensionale vectorruimte $(\mathbb{R},V,+)$ met spectrum $Spec(L) = \{\lambda_1,\lambda_2,...,\lambda_n\}$.
\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item $m(\lambda_i) \ge 1$\\
\item $d(\lambda_i) \ge 1$\\
\item $d(\lambda_i) \le m(\lambda_i)$\\w
\end{enumerate}
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door gefoefel.
Voor elke eigenwaarde $\lambda_i$ geldt het volgende.
\begin{enumerate}
\item Elke eigenwaarde heeft minstens een algebra\"ische multipliciteit van $1$. Anders zou het geen eigenwaarde zijn.

\item De eigenruimte van $\lambda_i$ is een vectorruimte van minstens dimensie $1$ omdat er voor elke eigenwaarde minstens $1$ eigenvector is, en dit niet de nulvector mag zijn.

\item
We weten dat de eigenruimte $E_{\lambda_i}$ van $\lambda_i$ dimensie $d(\lambda) = d$ heeft. Er bestaat dus een basis $\beta = v_1,v_2,...,v_d$ voor $E_\lambda$ met $d$ elementen. Omdat $\beta$ een vrij deel is van $E_\lambda$ en $V$ en omdat $E_\lambda$ een deelruimte is van V\footnote{Zie Definitie 5.14 p 188} geldt dat $\beta$ uitgebreid kan worden tot een basis van $V$\footnote{Zie Stelling 3.37 p 107}. Ten opzichte die uitgebreide basis $\beta'$ van $V$ ziet de matrix van $L$ er als volgt uit\footnote{Zie Stelling 5.7 p 181}. (Dit is het punt van eigenvectoren). ($L_{\beta'}^{\beta'}\cdot v_i$ moet $\lambda v_i$ zijn.)
\[
L_{\beta'}^{\beta'} = 
\begin{pmatrix}
\lambda & 0 & \cdots & 0 & \bullet & \cdots & \bullet\\
0 & \lambda & \cdots & 0 & \bullet & \cdots & \bullet\\
\vdots & \vdots & \ddots & \vdots & \vdots & &  \vdots\\
0 & 0 & \cdots & \lambda & \bullet & \cdots & \bullet\\
\vdots & \vdots & & \vdots & \vdots & \ddots & \vdots\\
0 & 0 &\cdots & 0 & \bullet & \cdots & \bullet\\
\end{pmatrix}
\]
We weten dat de karakteristieke veelterm onafhankelijk is van de gekozen basis\footnote{Zie Gevolg 5.9 p 182}. We beschouwen nu de karakteristieke veelterm van bovenstaande matrix.
De karakteristieke veelterm van deze matrix is dan van de volgende vorm. 
\[
\phi_L(X) = (X-\lambda)^dp(X)
\]
$m(\lambda)$ is minstens $d$ want $(X-\lambda)^d$ zorgt al voor multipliciteit $d$, maar in $p(X)$ kan $\lambda$ ook nog voorkomen. In symbolen:
\[
m(\lambda) \ge d = d(\lambda)
\]
\end{enumerate}
\end{proof}

\subsection{Stelling 5.18 p 190}
Zij $L:V\rightarrow V$ een lineaire transformatie van een eindigdimensionale vectorruimte $(\mathbb{R},V,+)$ en zij $\lambda_1,\lambda_2,...,\lambda_n$ verschillende eigenwaarden van $L$ met bijhorende eigenvectoren $v_1,v_2,...,v_n$.
\subsubsection*{Te Bewijzen}
$v_1,v_2,...,v_n$ zijn lineair onafhankelijke vectoren.
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door inductie op $n$.\\\\
\emph{Stap 1: (basis stap)}\\
De bewering geldt voor $n=1$ want $\{v_i\}$ is lineair onafhankelijk. Dit is waar omdat $v_i$ geen nulvector is\footnote{Zie Definitie 5.3 p 178}.\\\\
\emph{Stap 2: (inductie stap)}\\
Stel dat de bewering geldt voor een bepaalde $n=k$ (inductiehypothese). We bewijzen nu dat daaruit volgt dat de bewering geldt voor $n=k+1$.
We beschouwen $k+1$ eigenwaarden met bijhorende eigenvectoren. Uit de inductiehypothese volgt dat elke deelverzameling van $k$ elementen van die eigenvectoren vrij is.
We bewijzen uit het ongerijmde dat wanneer we de $k+1$-ste eigenvector toevoegen aan die deelverzameling, de verzameling nog steeds vrij is.
We nemen dus aan dat $v_{k+1}$ lineaire afhankelijk is van $v_1,v_2,...,v_k$ en proberen tot een contradictie te komen.
\[
\exists \mu_i\in\mathbb{R} : v_{k+1} = \sum_{i=1}^k\mu_iv_i
\]
Nemen we nu van beide kanten de afbeelding $L$ dan bekomen we de volgende vergelijking.
\[
L( v_{k+1}) = L\left(\sum_{i=1}^k\mu_iv_i\right)
\]
L is lineair\footnote{Zie Lemma 4.2 p 130} en $v_i$ zijn eigenvectoren met eigenwaarden $\lambda_i$ (gegeven).
\[
L(v_{k+1}) = \sum_{i=1}^k\mu_iL(v_i) = \sum_{i=1}^k\mu_i\lambda_iv_i
\]
Bovendien is $v_{k+1}$ ook een eigenvector van $L$.
\[
L(v_{k+1}) = \lambda_{k+1}v_{k+1} = \lambda_{k+1}\left(\sum_{i=1}^k\mu_iv_i\right) = \sum_{i=1}^k\lambda_{k+1}\mu_iv_i
\]
Voegen we deze twee nu samen dan krijgen we het volgende.
\[
\sum_{i=1}^k\mu_i\lambda_iv_i = \sum_{i=1}^k\lambda_{k+1}\mu_iv_i
\]
\[
\sum_{i=1}^k \mu_i(\lambda_i-\lambda_{k+1})v_i = 0
\]
Omdat we weten dat alle $\lambda_i$ onderling verschillend zijn (gegeven) en dat alle $v_i$ lineair onafhankelijk zijn houdt dit in dat alle $\mu_i$ nul moeten zijn. Dit zou betekenen dat alle $v_i$ lineair onafhankelijk zijn en dat is in contradictie met de aanname van lineair afhankelijkheid.
\end{proof}

\subsection{Gevolg 5.20 p 191}
Zij $L$ een lineaire transformatie van de $n$-dimensionale vectorruimte $(\mathbb{R},V,+)$.

\subsubsection*{Te Bewijzen}
\begin{center}
$L$ heeft een enkelvoudig spectrum $\Rightarrow$ $L$ is diagonaliseerbaar.
\end{center}

\subsubsection*{Bewijs}
\begin{proof}
Neem voor elke eigenwaarde van $L$ de bijhorende eigenvector. Elk van die eigenvectoren zijn lineair onafhankelijk\footnote{Zie Stelling 5.18 190}. Omdat $L$ een enkelvoudig spectrum heeft zijn dit precies $n$ eigenvectoren. Deze vectoren zijn bijgevolg ook voortbrengend voor $V$ en een basis van $V$\footnote{Zie Stelling 3.41 p 109}. Nu voldoet $L$ aan de definitie van een diagonaliseerbare lineaire transformatie.
\end{proof}


\subsection{Lemma 5.22 p 192}
Zij $L$ een diagonaliseerbare lineaire transformatie van de eindig dimensionale vectorruimte $(\mathbb{R},V,+)$.

\subsubsection*{Te Bewijzen}
De karakteristieke veelterm van $L$ is volledig te ontbinden als product van eerstegraadsfactoren.

\subsubsection*{Bewijs}
\begin{proof}
Omdat $L$ diagonaliseerbaar is bestaat er een inverteerbare $P$ zodat $B$ een diagonaalmatrix is in $L = P^{-1}BP$ \footnote{Zie het bewijs van Stelling 5.7 p 181}. Omdat de karakteristieke veelter onafhankelijk is van de basis is deze voor $L_A$ en $L_B$ gelijk \footnote{Zie Gevolg 5.9 p 182}. De karakteristieke veelterm van $L_B$ is dus $\prod_{i=1}^n(X-\lambda_i)$ met $\lambda_i$ de waarden op de diagonaal van $B$ (dit zijn de eigenwaarden van $L$). 
\end{proof}




\section{Oefeningen 5.9}


\end{document}